{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLiBxjBJ9IMs"
   },
   "source": [
    "# Урок 1. Алгоритм линейной регрессии. Градиентный спуск."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**План занятия**\n",
    "\n",
    "* [Теоретическая часть](#theory)\n",
    "    * [Основные понятия и обозначения классов](#0)\n",
    "    * [Линейная регрессия. MSE](#1)\n",
    "    * [Метод наименьших квадратов](#2)\n",
    "    * [Градиентный спуск](#3)\n",
    "* [Практическая часть](#practice)\n",
    "    * [Пример задачи](#practice)\n",
    "    * [Домашнее задание](#hw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Теоретическая часть<a class=\"anchor\" id=\"theory\"></a><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgfZohhE9IMz"
   },
   "source": [
    "## Основные понятия и обозначения<a class=\"anchor\" id=\"0\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCRpRG3-9IM0"
   },
   "source": [
    "В предыдущих курсах студенты уже знакомились с признаковыми описаниями, основными алгоритмами машинного обучения и Python-библиотеками, используемыми для решения задач в этой области. Однако, перед началом изучения программы курса давайте повторим основные понятия и обозначения, используемые в машинном обучении."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/ml.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучение с подкреплением** - испытуемая система (агент) обучается, взаимодействуя с некоторой средой.\n",
    "<table><tr>\n",
    "<td> <img src=\"images/openai1.gif\" style=\"width: 600px;\"/> </td>\n",
    "<td> <img src=\"images/unnamed.gif\" style=\"width: 450px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pzSl1NM9IM2"
   },
   "source": [
    "**Машинное обучение** - дисциплина, заключаящаяся в извлечении знаний из известных данных. Машинное обучение - это раздел математики, поэтому в нем мы будем, помимо всего прочего, работать с формулами.\n",
    "\n",
    "_Объект_ - то, для чего нужно сделать предсказание. Например, в задаче распознавания спам-почты объектом будет являться письмо. Объекты обозначаются буквой $x$. Множество всех объектов, для которых может потребоваться сделать предсказания, называется _пространством объектов_ и обозначается $\\mathbb{X}$.\n",
    "\n",
    "_Ответ_ - то, что нужно предсказать. В том же примере распознавания спама ответом будет является информация о том, является письмо спамом или нет. Ответы обозначаются буквой $y$ (можно сказать, что $y = y(x)$, так как ответ зависит от объекта). _Пространство ответов_ - множество всех ответов, с которыми мы можем работать. Оно обозначается $\\mathbb{Y}$. В примере задачи распознавания спама оно состоит из двух элементов: $+1$ и $-1$ (означающие, что письмо является и не является спамом, соответственно).\n",
    "\n",
    "Для реализации машинного обучения компьютеру нужно \"объяснить\" объекты, которые в первоначальном виде он понять не может, с помощью сущностей, ему понятных, например, чисел. Для этого вводится понятие _признаков_. Признак - это некая числовая характеристика объекта. Совокупность всех признаков объекта $x = (x^{1}, x^{2},..., x^{d})$ называется его _признаковым описанием_. Оно является $d$-мерным вектором, то есть к нему можно применять все операции, описанные линейной алгеброй."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/spam_data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eb5uTrO89IM4"
   },
   "source": [
    "Множество значений $i$-го признака будем обозначать $D_{i}$. Существует множество различных видов признаков:\n",
    "\n",
    "- _Бинарные признаки_ принимают два значения: $D_{i} = \\{0,1\\}$. Примером  в задаче кредитного скоринга может служить ответ, выше ли доход клиента определенной установленной суммы. При положительном ответе признак полагается равным 1, при отрицательном - 0.\n",
    "\n",
    "- _Вещественные признаки_ могут принимать в качестве значений все вещественные числа: $D_{i} = \\mathbb{R}$. Примерами могут выступать возраст человека, заработная плата, количество звонков в колл-центр в месяц и т.д.\n",
    "\n",
    "- _Категориальные признаки_ - это такие признаки, значения которых можно сравнивать только на равенство, и нельзя на \"больше-меньше\". В этом случае $D_{i}$ - неупорядоченное множетсво. Примерами таких признаков могут выступать город, в котором родился клиент банка, или его образование.\n",
    "\n",
    "- _Порядковые признаки_ - частный случай категориальных признаков. В этом случае $D_{i}$ - упорядоченное множество. Признаки можно сравнивать между собой, но нельзя определить расстояние между ними. Например, то же образование, но с введенным осмысленным порядком (высшее образование больше среднего профессионального, которое в свою очередь больше среднего и т.д.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_pQODZa09IM6"
   },
   "source": [
    "В первой части нашего курса мы рассмотрим алгоритмы _обучения с учителем_ или _контролироемого обучения (supervised learning)_. Данный метод заключается в восстановлении общей закономерности по конечному числу известных примеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iABsxWwB9IM6"
   },
   "source": [
    "Центральным понятием машинного обучения является _обучающая выборка_. Это примеры, на основе которых мы планируем строить общую закономерность. Она обозначается $X$ и состоит из $l$ пар объектов $x_{i}$ и известных ответов $y_{i}$:\n",
    "\n",
    "$$X = (x^{i}, y_{i})^l_{i=1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/spam_data.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F0Fh0yJn9IM7"
   },
   "source": [
    "Функция, отображающая пространство объектов $\\mathbb{X}$ в пространство ответов $\\mathbb{Y}$, помогающая нам делать предсказания, называется _алгоритмом_ или _моделью_ и обозначается $a(x)$. Она принимает на вход объект и выдает ответ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVlLfs339IM8"
   },
   "source": [
    "Для решения определенной задачи не все алгоритмы одинаково хорошо подходят. Для определения наиболее подходящего алгоритма введена характеристика, называемая _функционалом ошибки_ $Q(a, X)$. Он принимает на вход алгоритм и выборку и сообщает, насколько хорошо данный алгоритм работает на данной выборке. В примере распознавания спам-писем в качестве такого функционала может выступать доля неправильных ответов (предсказаний). Задача обучения заключается в подборе такого алгоритма, при котором достигается минимум функционала ошибки $Q(a, X)\\rightarrow min.$\n",
    "\n",
    "Наиболее подходящий алгоритм при этом выбирается из множества, называемого _семейством алгоритмов_ $\\mathbb{A}$. Их мы будем рассматривать в данном курсе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCfFle3N9INA"
   },
   "source": [
    "## Линейная регрессия. MSE<a class=\"anchor\" id=\"1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/linear_reg.png' width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-dd3ade75daef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ma1x1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ma2x2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ma3x3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ax' is not defined"
     ]
    }
   ],
   "source": [
    "y = ax + a1x1 + a2x2 + a3x3 + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bLvKfPfN9INB"
   },
   "source": [
    "Линейные модели - это такие модели, которые сводятся к суммированию значений признаков с некоторыми весами. Само название модели говорит о том, что зависимость предсказываемой переменной от признаков будет линейной:\n",
    "\n",
    "$$a(x) = w_{0}+\\sum^{d}_{i=1}w_{i}x_{i}.$$\n",
    "\n",
    "**Линейная регрессия** — модель зависимости переменной x от одной или нескольких других переменных (факторов, регрессоров, независимых переменных) с линейной функцией зависимости.\n",
    "\n",
    "В данном случае параметрами моделей являются веса $w_{i}$. Вес $w_{0}$ называется _свободным коэффициентом_ или _сдвигом_. Оптимизация модели в таком случае заключается в подборе оптимальных значений весов. Сумму в формуле также можно описать как скалярное произведение вектора признаков $x=(x^{1},...,x^{d})$ на вектор весов $w=(w_{1},...,w_{d})$:\n",
    "\n",
    "$$a(x) = w_{0}+\\left \\langle w,x \\right \\rangle.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/spam_data_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратим внимание, что сдвиг делает модель неоднородной и затрудняет ее дальнейшую оптимизацию. Для устранения этого фактора обычно используют прием, позволяющий упростить запись: к признаковому описанию объекта добавляется еще один признак (константный), на каждом объекте равный единице. В этом случае вес при нем как раз будет по смыслу совпадать со свободным коэффициентом, и сам $w_{0}$ будет не нужен. Тогда получим\n",
    "\n",
    "$$a(x) = \\sum^{d+1}_{i=1}w_{i}x_{i}=\\left \\langle w,x \\right \\rangle.$$\n",
    "\n",
    "За счет простой формы линейные модели достаточно легко обучаются и позволяют работать с зашумленными данными, небольшими выборками, контролирауя при этом риск переобучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|$$a(x)$$  |$$y$$    |$$a(x) - y$$ |$$|a(x) - y|$$ |$$(a(x)-y)^2$$|\n",
    "|:--------:|:-------:|:-----------:|:-------------:|:------------:|\n",
    "|11        |10       |1            |1              |1             |\n",
    "|9         |10       |-1           |1              |1             |\n",
    "|20        |10       |10           |10             |100           |\n",
    "|1         |10       |-9           |9              |81            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i8v9_4TB9INC"
   },
   "source": [
    "Для обучения модели необходимо иметь возможность измерять точность линейного алгоритма на выборке (обучающей или тестовой). \n",
    "\n",
    "В качестве меры ошибки можно взять абсолютное отклонение истинного значения от прогноза $Q(a,y)=a(x)-y$, но тогда минимизация функционала ошибки (в которой и состоит задача обучения) будет достигаться при принятии им отрицательного значения. Например, если истинное значение ответа равно $10$, а алгоритм $a(x)$ выдает ответ $11$, отклонение будет равно $1$, а при значении предсказания равном $1$, отклонение будет равно $1-10=-9$. Значение ошибки во втором случае ниже, однако разница истинного значения и предсказания нашей модели больше. Таким образом, такой функционал ошибки не поддается минимизации. \n",
    "\n",
    "Логичным кажется решение использовать в качестве функционала ошибки модуль отклонения $Q(a,y)=|a(x)-y|$. Соответствующий функционал ошибки называется средним абсолютным отклонением (mean absolute error, MAE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}|a(x_{i})-y_{i}|.$$\n",
    "\n",
    "Однако, как мы далее увидим, зачастую методы оптимизации включают в себя дифференцирование, а функция модуля не является гладкой - она не имеет производной в нуле, поэтому ее оптимизация бывает затруднительной.\n",
    "\n",
    "Поэтому сейчас основной способ измерить отклонение - посчитать квадрат разности $Q(a,y)=(a(x)-y)^{2}$. Такая функция является гладкой и имеет производную в каждой точке, а ее минимум достигается при равенстве истинного ответа $y$ и прогноза $a(x)$.\n",
    "\n",
    "Основанный на этой функции функционал ошибки называется _среднеквадратичным отклонением_ (mean squared error, MSE):\n",
    "\n",
    "$$Q(a,x) = \\frac{1}{l}\\sum^{l}_{i=1}(a(x_{i})-y_{i})^{2}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRwG-e0m9IND"
   },
   "source": [
    "## Метод наименьших квадратов<a class=\"anchor\" id=\"2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpzXCWHG9INE"
   },
   "source": [
    "Как уже говорилось ранее, обучение модели регрессии заключается в минимизации функционала ошибки. Таким образом, в случае использования среднеквадратичной ошибки получаем задачу оптимизации\n",
    "\n",
    "$$Q(w,x) = \\frac{1}{l}\\sum^{l}_{i=1}(\\left \\langle w,x_{i} \\right \\rangle-y_{i})^{2} \\rightarrow \\underset{w}{\\text{min}}.$$\n",
    "\n",
    "Способ вычисления весов путем минимизации среднеквадратичного отклонения называется **методом наименьших квадратов**.\n",
    "\n",
    "Заметим, что здесь мы переписали выражение функционала ошибки, заменив $a(x)$ на скалярное призведение $\\left \\langle w,x \\right \\rangle$, после чего мы уже имеем функцию, а не функционал ошибки, так как $Q$ зависит не от некоторой функции $a(x)$, а от вектора весов $w$, и оптимизировать нужно именно по нему, что гораздо проще."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eC50nWZ69INE"
   },
   "source": [
    "Имеет смысл переписать имеющиеся соотношения в матричном виде. В матрицу \"объекты-признаки\" впишем по строкам $d$ признаков для всех $l$ объектов из обучающей выборки: \n",
    "\n",
    "$$X = \\begin{pmatrix}\n",
    "x_{11} & ... & x_{1d}\\\\ \n",
    "... & ... & ...\\\\ \n",
    "x_{l1} & ... & x_{ld}\n",
    "\\end{pmatrix},$$\n",
    "\n",
    "и составим вектор ответов $y$ из истинных ответов для данной выборки:\n",
    "\n",
    "$$y = \\begin{pmatrix}\n",
    "y_{1}\\\\ \n",
    "...\\\\ \n",
    "y_{l}\n",
    "\\end{pmatrix}.$$\n",
    "\n",
    "Помня, что $w$ - вектор параметров, переписанная в матричном виде задача будет выглядеть следующим образом:\n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}},$$\n",
    "\n",
    "где используется евклидова ($L_{2}$) норма:\n",
    "\n",
    "$$||x|| = \\sqrt{\\sum_{i=1}^l{x_i^2}} $$\n",
    "\n",
    "$$||Xw-y|| = \\sqrt{\\sum_{i=1}^l{(X_iw-y_i)^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q(w, X) = \\frac{1}{l}\\sqrt{\\sum_{i=1}^l{(X_iw-y_i)^2}} ^{2} = \\frac{1}{l}\\sum_{i=1}^l{(X_iw-y_i)^2}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$\n",
    "\n",
    "$\\frac{2}{l}X^{T}(Xw-y) = 0$\n",
    "\n",
    "$X^{T}(Xw-y) = 0$\n",
    "\n",
    "$X^{T}Xw-X^{T}y = 0$\n",
    "\n",
    "$w = \\frac{X^{T}y}{X^{T}X}$\n",
    "\n",
    "$w = (X^{T}X)^{-1}X^{T}y $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1eW4aV609ING"
   },
   "source": [
    "Продифференцировав данную функцию по вектору $w$ и приравняв к нулю, можно получить явную анатилическую формулу для решения задачи минимизации ([ссылка  (см. пункт 1.2)](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) на подробный вывод аналитической формулы решения уравнения линейной регрессии):\n",
    "\n",
    "$$w = (X^{T}X)^{-1}X^{T}y.$$\n",
    "\n",
    "Это решение называется _нормальным уравнением_ линейной регрессии. Наличие аналитического решения кажется положительным фактором, однако, у него есть некоторые минусы, среди которых вычислительная сложность операции (обращение матрицы $X^{T}X$ будет иметь кубическую сложность от количества признаков $d^{3}$), а также тот факт, что матрица $X^{T}X$ может быть вырожденной и поэтому необратимой. Тогда найти решение будет невозможно.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C63Alg-A9INH"
   },
   "source": [
    "Более удобным подходом будет разработка решения с помощью численных методов оптимизации, одним из которых является _градиентный спуск_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfRgH2cC9INI"
   },
   "source": [
    "## Градиентный спуск<a class=\"anchor\" id=\"3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgHZFo-x9INK"
   },
   "source": [
    "Среднеквадратичная ошибка имеет один минимум и непрерывна на всей области значений (то есть является выпуклой и гладкой), а значит в каждой ее точке можно посчитать частные производные.\n",
    "\n",
    "Вспомним, что _градиентом_ функции $f$ называется $n$-мерный вектор из частных производных. \n",
    "\n",
    "$$ \\nabla f(x_{1},...,x_{d}) = \\left(\\frac{\\partial f}{\\partial x_{i}}\\right)^{d}_{i=1}.$$\n",
    "\n",
    "При этом известно, что __градиент задает направление наискорейшего роста функции__. Значит, антиградиент будет показывать направление ее скорейшего убывания, что будет полезно нам в нашей задаче минимизации функционала ошибки. \n",
    "\n",
    "**Градиентный спуск** — метод нахождения локального экстремума функции (минимума или максимума) с помощью движения вдоль градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x1, x2, x3) = (df(x1), df(x2), df(x3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EITeLDLk9INM"
   },
   "source": [
    "Для решения задачи нам требуется определить некоторую стартовую точку и итерационно сдвигаться от нее в сторону антиградиента с определенным _шагом_ $\\eta_{k}$, на каждом шагу пересчитывая градиент в точке, в которой мы находимся. Таким образом, имея начальный вектор весов $w^{0}$, $k$-й шаг градиентного спуска будет иметь вид\n",
    "\n",
    "$$w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X).$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AD9YpghL9INO"
   },
   "source": [
    "Итерации следует продолжать, пока не наступает сходимость. Она определяется разными способами, но в даннном случае удобно определять как ситуацию, когда векторы весов от шага к шагу изменяются незначительно, то есть норма отклонения вектора весов на текущем шаге от предыдущего не привышает заданное значение $\\varepsilon$:\n",
    "\n",
    "$$||w^{k}-w^{k-1}|| < \\varepsilon.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/gradient_descent_lr.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4yKBld_9INT"
   },
   "source": [
    "Начальный вектор весов $w_{0}$ также можно определять различными способами, обычно его берут нулевым или состоящим из случайных небольших чисел."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhuJyO__9INU"
   },
   "source": [
    "В случае многомерной регрессии (при количестве признаков больше 1) при оптимизации функционала ошибки \n",
    "\n",
    "$$Q(w, X) = \\frac{1}{l}||Xw-y||^{2}\\rightarrow \\underset{w}{\\text{min}}$$\n",
    "\n",
    "формула вычисления градиента принимает вид\n",
    "\n",
    "$$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. w0 = 0\n",
    "2. k = 1:\n",
    "        w1 = w0 - grad(w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X)$\n",
    "\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/gradient_descent_lr.png' width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Практическая часть<a class=\"anchor\" id=\"practice\"></a><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkskCJdb9INV"
   },
   "source": [
    "Смоделируем работу градиентного спуска при помощи Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJ1KA9cq9INW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.axes3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lKUBgDlf9INe"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "# Возьмем 2 признака и 1000 объектов\n",
    "n_features = 2\n",
    "n_objects = 1000\n",
    "\n",
    "# сгенерируем вектор истинных весов\n",
    "w_true = np.random.normal(size=(n_features))\n",
    "\n",
    "# сгенерируем матрицу X, вычислим Y с добавлением случайного шума\n",
    "X = np.random.uniform(-7, 7, (n_objects, n_features))\n",
    "Y = X.dot(w_true) + np.random.normal(0, 0.5, size=(n_objects))\n",
    "\n",
    "# возьмем нулевые начальные веса\n",
    "w = np.zeros(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(w, w_true, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], Y)\n",
    "\n",
    "ax.set_xlabel('X0')\n",
    "ax.set_ylabel('X1')\n",
    "ax.set_zlabel('Y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ujy3MY_s9INj"
   },
   "outputs": [],
   "source": [
    "# реализуем функцию, определяющую среднеквадратичную ошибку\n",
    "def mserror(X, w, y):\n",
    "    y_pred = X.dot(w)\n",
    "    return (np.sum((y_pred - y)**2)) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeKE8moY9INm"
   },
   "source": [
    "Реализуем функцию, вычисляющую вектор весов по нормальному уравнению линейной регрессии, и применим ее.\n",
    "$$w = (X^{T}X)^{-1}X^{T}y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpxGlgkD9INn",
    "outputId": "dfff8dca-6f99-4ebf-8921-c1fa156f0a05"
   },
   "outputs": [],
   "source": [
    "normal_eq_w = np.linalg.inv(np.dot(X.T, X)) @ X.T @ Y\n",
    "print(f'Веса {normal_eq_w}')\n",
    "print(f'В случае использования нормального уравнения функционал ошибки составляет ', end='')\n",
    "print(f'{round(mserror(X, normal_eq_w, Y), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cqw7Fffj9IN0"
   },
   "source": [
    "Обучим линейную регрессию путем градиентного спуска и получим графики изменения весов и ошибки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2vnzIkxt9IN1",
    "outputId": "28f71598-e128-4adb-da62-1601401800f1"
   },
   "outputs": [],
   "source": [
    "# список векторов весов после каждой итерации\n",
    "w_list = [w.copy()]\n",
    "\n",
    "# список значений ошибок после каждой итерации\n",
    "errors = []\n",
    "\n",
    "# шаг градиентного спуска\n",
    "eta = 0.01\n",
    "\n",
    "# максимальное число итераций\n",
    "max_iter = 1e4\n",
    "\n",
    "# критерий сходимости (разница весов, при которой алгоритм останавливается)\n",
    "min_weight_dist = 1e-8\n",
    "\n",
    "# зададим начальную разницу весов большим числом\n",
    "weight_dist = np.inf\n",
    "\n",
    "# счетчик итераций\n",
    "iter_num = 0\n",
    "\n",
    "# ход градиентного спуска\n",
    "while weight_dist > min_weight_dist and iter_num < max_iter:\n",
    "    y_pred = np.dot(X, w)\n",
    "    dQ = 2 / Y.shape[0] * np.dot(X.T, y_pred - Y)\n",
    "    new_w = w - eta * dQ\n",
    "    weight_dist = np.linalg.norm(new_w - w, ord=2)\n",
    "    error = mserror(X, new_w, Y)\n",
    "    \n",
    "    w_list.append(new_w.copy())\n",
    "    errors.append(error)\n",
    "    \n",
    "    print(f'Iter {iter_num}: error - {error}, weights: {new_w}')\n",
    "    \n",
    "    iter_num += 1\n",
    "    w = new_w\n",
    "    \n",
    "w_list = np.array(w_list)\n",
    "w_pred = w_list[-1]\n",
    "\n",
    "print(f'В случае использования градиентного спуска функционал ошибки составляет {round(errors[-1], 4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Afahfb849IN4",
    "outputId": "f1eb65a2-f819-49d3-8cb6-491d06b01d49"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение весов (красной точкой обозначены истинные веса, сгенерированные вначале)\n",
    "plt.figure(figsize=(13, 6))\n",
    "plt.title('Gradient descent')\n",
    "plt.xlabel(r'$w_1$')\n",
    "plt.ylabel(r'$w_2$')\n",
    "\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.scatter(w_true[0], w_true[1], c='r')\n",
    "plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MiTINBlK9IN7"
   },
   "source": [
    "После каждой итерации значения искомых весов приближаются к истинным, однако, не становятся им равны из-за шума, добавленного в вектор ответов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFzwe8pW9IN8",
    "outputId": "511c62c8-8ee3-40cc-ac09-be80875cc9ab"
   },
   "outputs": [],
   "source": [
    "# Визуализируем изменение функционала ошибки\n",
    "plt.plot(range(len(errors)), errors)\n",
    "plt.title('MSE')\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Kcz4wFx9IOB"
   },
   "source": [
    "Видно, что изменение монотонно и начинается с высокой точки, после определенного количества итераций выходя на асимптоту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(-3.0, 4.0, 0.05).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_manual(w1, w2, y_pred):\n",
    "    w = np.array([w1, w2])\n",
    "    y = X.dot(w)\n",
    "    return (sum((y - y_pred)**2)) / len(y)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "w1 = w2 = np.arange(-3.0, 4.0, 0.05)\n",
    "w1, w2 = np.meshgrid(w1, w2)\n",
    "\n",
    "zs = np.array([mse_manual(i, j, Y) for i, j in zip(np.ravel(w1), np.ravel(w2))])\n",
    "Z = zs.reshape(w1.shape)\n",
    "\n",
    "ax.scatter(w_true[0], w_true[1], mse_manual(w_true[0], w_true[1], Y), c='r', s=10)\n",
    "ax.scatter(w_pred[0], w_pred[1], mse_manual(w_pred[0], w_pred[1], Y), c='g', s=10)\n",
    "ax.plot_surface(w1, w2, Z, alpha=.5)\n",
    "\n",
    "ax.set_xlabel(r'$w_1$')\n",
    "ax.set_ylabel(r'$w_2$')\n",
    "ax.set_zlabel('MSE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "noIKD8ls9IOD"
   },
   "source": [
    "Очень важно при использовании метода градиентного спуска правильно подбирать шаг. Если длина шага будет слишком мала, то метод будет слишком медленно приближаться к правильному ответу, и потребуется очень большое количество итераций для достижения сходимости. Если же длина наоборот будет слишком большой, появится вероятность \"перепрыгивания\" алгоритма через минимум функции или вообще отсутствия сходимости градиентного спуска.\n",
    "\n",
    "Применяется методика использования переменного размера шага: на начальных этапах берется большой шаг, который с увеличением количества итераций снижается. Одна из таких методик - вычисление на каждой итерации размера шага по формуле\n",
    "\n",
    "$$\\eta_{k} = \\frac{c}{k},$$\n",
    "\n",
    "где $c$ - некоторая константа, а $k$ - номер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример задачи<a class=\"anchor\" id=\"example\"></a><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Задача:__ предсказание баллов ЕГЭ ученика в зависимости от кол-ва лет стажа его репетитора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i77tZbAd5plB"
   },
   "outputs": [],
   "source": [
    "X = np.array([[ 1,  1],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2],\n",
    "              [ 1,  5],\n",
    "              [ 1,  3],\n",
    "              [ 1,  0],\n",
    "              [ 1,  5],\n",
    "              [ 1, 10],\n",
    "              [ 1,  1],\n",
    "              [ 1,  2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ejifaMBe6VaP",
    "outputId": "190a541b-5c6d-48f6-8982-b513de5d7606"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsNrKi1Q6Wmh"
   },
   "outputs": [],
   "source": [
    "y = [45, 55, 50, 55, 60, 35, 75, 80, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уравнение прямой: $y = a*x + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NntLxvtU7CbH"
   },
   "outputs": [],
   "source": [
    "y_pred1 = 5 * X[:, 1] + 35 * X[:, 0] \n",
    "y_pred2 = 7.5 * X[:, 1] + 40 * X[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "OTikrodr6n-i",
    "outputId": "9c347927-ae0d-40ba-bcd7-fdd9de2688bf"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], y_pred1, label='1')\n",
    "plt.plot(X[:, 1], y_pred2, label='2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отклонение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbsqjAxH9WqI"
   },
   "outputs": [],
   "source": [
    "err1 = np.sum(y - y_pred1)\n",
    "err2 = np.sum(y - y_pred2)\n",
    "err1, err2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAE (Mean Absolute Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUqCthPf9eYI"
   },
   "outputs": [],
   "source": [
    "mae_1 = np.mean(np.abs(y - y_pred1))\n",
    "mae_2 = np.mean(np.abs(y - y_pred2))\n",
    "mae_1, mae_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTn2bZIg_EBS"
   },
   "outputs": [],
   "source": [
    "mse_1 = np.mean((y - y_pred1)**2)\n",
    "mse_2 = np.mean((y - y_pred2)**2)\n",
    "mse_1, mse_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4zHyZ2Da5HV"
   },
   "source": [
    "### Метод наименьших квадратов (МНК)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w = (X^{T}X)^{-1}X^{T}y.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mnyZzkaDBegD",
    "outputId": "368800a6-deef-46d0-b27f-d6eb5e46a743"
   },
   "outputs": [],
   "source": [
    "W_analytical = np.linalg.inv(np.dot(X.T, X)) @ X.T @ y\n",
    "W_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_analytical = W_analytical[0] * X[:, 0] + W_analytical[1] * X[:, 1]\n",
    "y_pred_analytical = X @ W_analytical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "Lyg-J3H7Bnv5",
    "outputId": "7701dd28-d123-4773-cd1f-1439abfb6d90"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], y_pred1, label='1 - manual')\n",
    "plt.plot(X[:, 1], y_pred2, label='2 - manual')\n",
    "plt.plot(X[:, 1], y_pred_analytical, label='3 - analytical solution')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTSC2cFnCd8T"
   },
   "outputs": [],
   "source": [
    "def calc_mae(y, y_pred):\n",
    "    err = np.mean(np.abs(y - y_pred))\n",
    "    return err\n",
    "\n",
    "def calc_mse(y, y_pred):\n",
    "    err = np.mean((y - y_pred)**2)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MYSrwISJDGfS",
    "outputId": "891bfd3a-142c-466c-a9d9-7ebfb303e86c"
   },
   "outputs": [],
   "source": [
    "calc_mae(y, y_pred1), calc_mse(y, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MYSrwISJDGfS",
    "outputId": "891bfd3a-142c-466c-a9d9-7ebfb303e86c"
   },
   "outputs": [],
   "source": [
    "calc_mae(y, y_pred2), calc_mse(y, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_WtO6lvhDmQ6",
    "outputId": "8de8b895-85d8-448e-f565-b70e93a5dd25"
   },
   "outputs": [],
   "source": [
    "calc_mae(y, y_pred_analytical), calc_mse(y, y_pred_analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAOgIXLRa--g"
   },
   "source": [
    "### Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhuJyO__9INU"
   },
   "source": [
    "$$\\nabla_{w}Q(w,X) = \\frac{2}{l}X^{T}(Xw-y).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Инициализация w\n",
    "\n",
    "2. Цикл по k = 1,2,3,...:\n",
    "\n",
    "    * $w^{k} = w^{k-1} - \\eta_{k}\\nabla Q(w^{k-1}, X)$\n",
    "\n",
    "    * Если $||w^{k} - w^{k-1}|| < \\epsilon$, то завершить.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.normal(size=(X.shape[1]))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-2 # величина шага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape,  W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNebwUP7Fd8F"
   },
   "outputs": [],
   "source": [
    "n = len(y)\n",
    "dQ = 2/n * X.T @ (X @ W - y) # градиент функции ошибки\n",
    "dQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DKkvmkwHdOx"
   },
   "outputs": [],
   "source": [
    "grad = eta * dQ\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZVRCF5P2MYI6",
    "outputId": "0b0e2f05-a23c-4bf8-f37d-b4240d2e0af0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'previous weights', W)\n",
    "W = W - grad\n",
    "print(f'new weights', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_grad = X @ W\n",
    "plt.scatter(X[:, 1], y)\n",
    "plt.plot(X[:, 1], y_pred_grad, label='gradient descent', c='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание <a class=\"anchor\" id=\"hw\"></a><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huXrhXQsZTMt"
   },
   "source": [
    "1. Подберите скорость обучения (eta) и количество итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "\n",
    "eta = 0.05 \n",
    "n_iter = 100\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "IDB22MQKMYaJ",
    "outputId": "4c03219e-a57c-4583-f439-6699fd0619bb"
   },
   "outputs": [],
   "source": [
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "    for k in range(W.shape[0]):\n",
    "        W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "    if i % 10 == 0:\n",
    "        eta /= 1.1\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# при eta = 0.05 достаточно 100 итераций (n_iter) для получения результата с MSE = 44.31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Qu1o4JhZYwI"
   },
   "source": [
    "2*. В этом коде мы избавляемся от итераций по весам, но тут есть ошибка, исправьте ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "\n",
    "eta = 1e-2 \n",
    "n_iter = 100\n",
    "\n",
    "W = np.array([1, 0.5])\n",
    "print(f'Number of objects = {n} \\\n",
    "       \\nLearning rate = {eta} \\\n",
    "       \\nInitial weights = {W} \\n')\n",
    "\n",
    "for i in range(n_iter):\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "#     for k in range(W.shape[0]):\n",
    "#         W[k] -= eta * (1/n * 2 * X[:, k] @ (y_pred - y))\n",
    "    \n",
    "    # БЫЛО ТАК:\n",
    "    # W -= eta * (1/n * 2 * np.dot(X, y_pred - y))\n",
    "    \n",
    "    # НУЖНО ТАК: \n",
    "    W -= eta * (1/n * 2 * np.sum(X * (y_pred - y).reshape(X.shape[0], -1), axis=0))    \n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f'Iteration #{i}: W_new = {W}, MSE = {round(err,2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3*. Вместо того, чтобы задавать количество итераций, задайте другое условие останова алгоритма - когда веса перестают изменяться меньше определенного порога $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[1]\n",
    "ect = 0.0001 # ect - error change threshold\n",
    "eta = 0.01\n",
    "W = np.array([1, 0.5])\n",
    "W, eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "while True:\n",
    "    y_pred = np.dot(X, W)\n",
    "    err = calc_mse(y, y_pred)\n",
    "    if i > 1:\n",
    "        if abs(err - err_prev) < ect:\n",
    "            print(f'# итерация: {i}, MSE: {err}, веса: {W}')\n",
    "            break\n",
    "    for j in range(W.shape[0]):\n",
    "        W[j] -= eta * (1/n * 2 * np.sum(X[:,j] * (y_pred - y)))\n",
    "    if i % 100 == 0:\n",
    "        print(f'# итерация: {i}, MSE: {err}, веса: {W}')\n",
    "    err_prev = err\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# при пороге изменения ошибки (ect) ниже 0.0001 получаем результат MSE = 45.01629117 за 147 итераций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMF9JaD09IOF"
   },
   "source": [
    "## Дополнительные материалы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7td81sE9IOG"
   },
   "source": [
    "1. [Вывод аналитической формулы решения уравнения линейной регрессии](https://habr.com/ru/company/ods/blog/323890/#metod-naimenshih-kvadratov) (см. пункт 1.2)\n",
    "2. [Математическое описание метода градиентного спуска](http://www.machinelearning.ru/wiki/index.php?title=%D0%9C%D0%B5%D1%82%D0%BE%D0%B4_%D0%B3%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D0%BE%D0%B3%D0%BE_%D1%81%D0%BF%D1%83%D1%81%D0%BA%D0%B0)\n",
    "3. [Документация NumPy](https://docs.scipy.org/doc/numpy-1.16.0/reference/routines.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuMdnCuu9IOI"
   },
   "source": [
    "* Линейная регрессия - простой, но зачастую эффективный, способ приближать вещественную целевую переменную через линейную комбинацию признаков\n",
    "* Решение регрессии - МНК, можно решать аналитически, но на практике - градиентный спуск (GD, Gradient Descent)\n",
    "* MSE удобна для градиентного спуска, так как дифференцируема\n",
    "* Максимальное качество градиентного спуска достигается малыми шагами и большим кол-вом итераций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Опеределения\n",
    "*Машинное обучение*\n",
    "\n",
    "**Машинное обучение** — дисциплина, заключающаяся в извлечении знаний из известных данных.\n",
    "\n",
    "**Признак** — это индивидуальное измеримое свойство или характеристика наблюдения.\n",
    "\n",
    "**Обучающая выборка** — набор структурированных данных, используемый для обучения моделей.\n",
    "\n",
    "**Обучение с учителем** — это направление машинного обучения, объединяющее алгоритмы и методы построения моделей на основе множества примеров, содержащих пары \"известный вход - известный выход\".\n",
    "\n",
    "**Обучение без учителя** — направление машинного обучения, в которой для коррекции параметров обучаемой модели не используется целевая функция. Иными словами, в обучающих примерах при обучении без учителя не нужно иметь заранее заданные выходы модели.\n",
    "\n",
    "**Обучение с подкреплением** — раздел машинного обучения, изучающий поведение интеллектуальных агентов, действующих в некоторой среде и принимающих решения.\n",
    "___________\n",
    "\n",
    "_Линейные модели_\n",
    "\n",
    "**Линейные модели** — такие модели, которые сводятся к суммированию значений признаков с некоторыми весами\n",
    "\n",
    "**Линейная регрессия** — модель зависимости переменной x от одной или нескольких других переменных (факторов, регрессоров, независимых переменных) с линейной функцией зависимости.\n",
    "\n",
    "**Метод наименьших квадратов** — способ вычисления весов линейной модели путем минимизации среднеквадратичного отклонения.\n",
    "___________\n",
    "\n",
    "_Градиентный спуск_\n",
    "\n",
    "**Градиент функции** — $n$-мерный вектор из частных производных. Задает направление наискорейшего роста функции. $$ \\nabla f(x_{1},...,x_{d}) = \\left(\\frac{\\partial f}{\\partial x_{i}}\\right)^{d}_{i=1}.$$\n",
    "\n",
    "**Градиентный спуск** — метод нахождения локального экстремума функции (минимума или максимума) с помощью движения вдоль градиента."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lesson_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
